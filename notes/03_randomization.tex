\section{Randomization}

In the previous sections, we have only dealt with (non)-deterministic TMs. However, there are many important classes that relate a TM when using a probability, as we define below:

\begin{definition}
A \emph{probabilistic TM} (PTM) has two transition functions, and at each step we choose, independently with probability $\frac{1}{2}$, to apply one of the two functions, like a coin flip. We say that a PTM $M$ halts in $T(n)$ time if $M$ halts on all inputs $x$ within $T(|x|)$ steps, regardless of the choices made during execution.
\end{definition}

We can now define the probabilistic analog of $\P$, which is $\BPP$:
\begin{definition}
A language $L$ is in $\BPTIME(T(n))$ if there exists a PTM $M$ that decides $L$ in time $T(n)$, $M$ halts within $T(|x|)$ steps regardless of choices, and $\Pr[M(x) = L(x)] \ge \frac{2}{3}$, where $L(x) = 1$ if $x \in L$, and $L(x) = 0$ if $x \notin L$. We define:
\[
\BPP = \bigcup_{c \ge 0}\BPTIME(n^c)
\]
\end{definition}

We also give an alternate definition for $\BPP$:
\begin{definition}
$L \in \BPP$ if there exists a poly-time TM verifier $M$ and a polynomial $p$ such that for all $x \in \{0, 1\}^*$, $\Pr[L(M) = L] \ge \frac{2}{3}$ where $M$ has certificates of size $p(|x|)$.
\end{definition}

\begin{theorem}
$\BPP \subseteq \Ppoly$.
\end{theorem}

This proof will use what is called as ``success amplification" - the essential idea is that, given we have polynomial time, we can make successive coin flips to reduce the error bound for $\BPP$ to even less error. We do this as follows:
\begin{itemize}
\item For a PTM $M$ that computes a language $L$ with error $\frac{1}{2} - \frac{1}{n^c}$, run $M$ for $k$ independent trials, and accept only if at least half of the trials accept. 
\item Define $X_i = 1$ if the $i$-th trial accepts, and 0 otherwise, and $X = \Sigma_1^k X_i$. 
\item Therefore, $\Pr[X_i = 1] \ge \frac{1}{2} + \frac{1}{n^c}$. Call this result $p$.
\item $\E[X] \ge pk$ for the same reasoning.
\item Call $\delta = 1-\frac{1}{2p}$.
\end{itemize}
Therefore, now we want to find the probability that the trials done above are incorrect, by using Chernoff bounds:
\begin{center}
$\Pr[X < (1-\delta)pk] \le \exp(\frac{-{(1-\frac{1}{2p})^2}pk}{2}) \le \exp(-\frac{k}{2n^{2c}})$
\end{center}
So, for $k$ a polynomial (i.e., $2n^{2c+d}$), we can have an exponentially small error bound. Now, onto the proof.

\begin{proof}
Let $L \in \BPP$. By success amplification, there is a poly-time TM $M$ such that $L(M) = L$ with error smaller than $2^{-n}$. Choose an $x \in \{0, 1\}^n$. Therefore, $\Pr[\text{$M$ computes incorrectly on $x$}] < \frac{1}{2^n}$. We can observe that:
\[
\Pr[\text{$M$ computes incorrectly on some $y \in \{0, 1\}^n$}] \le \Sigma_{x \in \{0, 1\}^n} \Pr[\text{$M$ computes incorrectly on $x$}] = 1
\]
Therefore, there are some coin flips where $M$ does not make any error on any $x \in \{0, 1\}^n$. All we need to do is convert $M$ into a circuit and hardwire these coin flips, which completes the proof. 
\end{proof}

\begin{theorem}[Sipser-G\'{a}cs]
$\BPP \subseteq \Sigma_2^\P \cap \Pi_2^\P$.
\end{theorem}

\begin{proof}
As before, assume that for an $L \in \BPP$ there is a poly-time TM $M$ such that $L(M) = L$ with error smaller than $2^{-n}$. Since $\BPP$ is closed under complement, we only need to show $\BPP \subseteq \Sigma_2^\P$.

\par For $x \in \{0, 1\}^n$, let $S_x$ be the set of certificates for which $M$ accepts $x$, and let $p$ be the polynomial that is the number of random bits $M$ uses. Therefore, either $|S_x| \ge (1-2^{-n})2^{p(n)}$, or $|S_x| \le 2^{p(n)-n}$, whether $x \in L$ or not. What we will show is that we can differentiate between these using only 2 quantifiers.

\par For $A \subseteq \{0, 1\}^{p(n)}$ and $b \in \{0, 1\}^{p(n)}$, define $A+b = \{a+b \colon a \in A\}$, where the operation is done $(\mod 2)$. Let $k = \lceil {p(n)/n} \rceil +1$. We will show the following:

\begin{enumerate}
\item For all $S \subseteq \{0, 1\}^{p(n)}$ and $|S| \le 2^{p(n)-n}$ and any $k$ vectors $\{u_i\}_{1 \le i \le k}$ with $u_i \in \{0, 1\}^{p(n)}$, we have that $\bigcup_{i=1}^{k} (S+u_i) \ne \{0, 1\}^{p(n)}$.

\item For all $S \subseteq \{0, 1\}^{p(n)}$ and $|S| \ge (1-2^{-n})2^{p(n)}$, there exist $k$ vectors $\{u_i\}_{1 \le i \le k}$ with $\bigcup_{i=1}^{k}(S+u_i) = \{0, 1\}^{p(n)}$.
\end{enumerate}
In other words, we can establish that $x \in L$ with only 2 quantifiers. Proving these implies that $\BPP \subseteq \Sigma_2^\P \cap \Pi_2^\P$. For the first proof, we have that $|S + u_i| = |S|$, so the union is at most $k|S| < 2^{p(n)}$ for large enough $n$.

\par For the second proof, if the $u_i$ are chosen independently randomly, then $\Pr[\bigcup_{i=1}^{k}(S+u_i) = \{0, 1\}] > 0$ will imply the proof. Let $B_r$ be the event that $r \notin \bigcup_{i=1}^{k}(S+u_i)$. This implies that we need to prove that $\Pr[\exists B_r] < 1$ for some $r \in \{0, 1\}^{p(n)}$. Equivalently, we can show that $\Pr[B_r] < 2^{-p(n)}$ for all $r$. 

\par Define $B_r^i$ the event that $r \notin S + u_i$ (equivalently, $r + u_i \notin S$). We can see that $B_r = \bigcap_{1 \le i \le k}B_{r}^i$. However, $r+u_i$ is uniformly chosen from $\{0, 1\}^{p(n)}$, so $r+u_i \in S$ with probability $\ge 1-2^{-n}$. Also, since the $B_r^i$ are independent for all $i$, we have $\Pr[B_r] = (\Pr[B_r^i])^k \le 2^{-nk} < 2^{-p(n)}$. 
\end{proof}

\subsection{$\RP, \coRP, \ZPP$}
Now we look at ``one-sided" error classes, where before we had $\BPP$ which was a randomized class with two sides of error (the probability of outputting ``accept" given that $x \in L$ was strictly less than 1, and that of ``reject" given $x \notin L$ was strictly greater than 0). 

\newcommand{\RTIME}{\lang{RTIME}}
\newcommand{\ZTIME}{\lang{ZTIME}}
\begin{definition}
We have $L \in \RTIME(T(n))$ if there is a $T(n)$-time PTM $M$ such that if $x \in L$, then $\Pr[M(x) = 1] \ge \frac{2}{3}$, and if $x \notin L$, then $\Pr[M(x) = 0] = 0$. Define $\RP = \bigcup_{c \ge 0}\RTIME(n^c)$. 
\end{definition}

We can also see that $\coRP$ has the ``other side" error. In order to define a complexity class with no error for a PTM (i.e., $\ZPP$), we need to define ``expected running time":

\begin{definition}
Let $M$ be a PTM with input $x$. Let $T_{M,x}$ be the running time of $M$ on $x$. Therefore, $\Pr[T_{M, x} = T] = p$ has that $M$ halts on $x$ within $T$ steps with probability $p$. We define $M$ to have \emph{expected running time T(n)} if $\mathbb{E}[T_{M, x}] \le T(|x|)$ for all $x$. 
\end{definition}

\begin{definition}
We have $L \in \ZTIME(T(n))$ when there is a PTM $M$ that runs in expected time $O(T(n))$ where for all $x$, $M(x) = L(x)$ (i.e., no error). Define $\ZPP = \bigcup_{c \ge 0}\ZTIME(n^c)$. 
\end{definition}

So how do these 3 randomized classes relate to each other? In fact, we will prove the following:
\begin{theorem}
$\ZPP = \RP \cap \coRP$. 
\end{theorem}

We can observe that if $x \in L$, then $M$ on $x$ outputs 1 or \texttt{?} (i.e., it does not know); also, if $x \notin L$, then $M$ on $x$ outputs 0 or \texttt{?}. If $M$ does not output \texttt{?}, it did not give an incorrect answer (and if it is \texttt{?}, it has a $\frac{1}{2}$ chance of getting the answer correct by choosing randomly). This is important for the proof. 

\begin{proof}
We need to prove that $\ZPP \subseteq \RP$ and $\ZPP \subseteq \coRP$; we prove the latter since the former is very similar. So suppose $L \in \ZPP$; by definition, there exists a PTM $M$ that correctly decides that $x \in L$ or outputs \texttt{?}. Let $M'$ be a TM that on $x$, outputs ``accept" if $M$ on $x$ outputs \texttt{?}; otherwise, it outputs $M$'s output.

\par If $x \in L$, we have $M'$ on $x$ outputs 1. If $x \notin L$, then either $M$ outputs 0 (correctly) or \texttt{?} where $M'$'s output is incorrect with probability $\le \frac{1}{2}$. Therefore, by definition, $L \in \coRP$, which has that $\ZPP \subseteq \coRP$.

\par For showing $\RP \cap \coRP \subseteq \ZPP$, let $L \in \RP \cap \coRP$. Therefore (for both classes), there exist two TMs $A, B$ such that if $x \in L$, then $A(x) = 1$, and is incorrect for $x \notin L$ with probability $\le \frac{1}{2}$, and if $x \notin L$, then $B(x) = 0$, and is incorrect for $x \in L$ with probability $\le \frac{1}{2}$. 

\par We construct a PTM $M$ that does the following:
\begin{enumerate}
\item Run $A$ on $x$, and if $A(x) = 0$, return the result of $x \notin L$.
\item Run $B$ on $x$, and if $B(x) = 1$, return the result of $x \in L$. 
\item Otherwise, return \texttt{?}.
\end{enumerate}
We see that if $M(x)$ does not return \texttt{?}, the result is correct. Also, we have $M(x)$ returning \texttt{?} with probability $\le \frac{1}{2}$ by the following argument: assume $x \in L$ (the $x \notin L$ case is similar). Therefore, we don't return 1 if and only if $B(x) = 0$; we also have that $B(x) = 0$ for $x \in L$ with probability $\le \frac{1}{2}$. Therefore, $L \in \ZPP$, which shows that $\RP \cap \coRP \subseteq \ZPP$.

\par Therefore, we have $\ZPP = \RP \cap \coRP$.  
\end{proof}

\subsection{Randomized Reductions}
So we have various randomized classes - what is the analog of a reduction in these? We have deterministic ones that we have seen in $\NP$ and other classes. We now define a randomized reduction:
\begin{definition}
We say that $A$ \emph{randomly reduces} to $B$ ($B \le_r C$) if there is a poly-time PTM $M$ such that for all $x$, $\Pr[B(M(x)) = C(x)] \ge \frac{2}{3}$. 
\end{definition}
Unlike the reductions we have seen before, this reduction is not transitive. However, $\BPP$ is closed under randomized reductions (i.e., if $B \le_r C$ and $C \in \BPP$, then so is $B$). To use randomized reductions, we create a class that randomly reduces from the deterministic 3SAT:
\begin{definition}
$\BP\cdot \NP = \{L \colon L \le_r \ThreeSAT\}$. 
\end{definition}
Note that this is quite different than $\NP$. There is even a large difference between $\ThreeSAT$ and $\overline{\ThreeSAT}$:
\begin{theorem}
If $\ThreeSAT \in \BP\cdot \NP$, then $\PH = \Sigma_3^\P$. 
\end{theorem}  

\newcommand{\UPATH}{\lang{UPATH}}
\subsection{$\BPL, \RL$}
We develop an analog of $\BPP, \RP$ in the same way but for log-space:
\begin{definition}
We have $L \in \BPL$ if there is an $O(\log(n))$-space PTM $M$ such that $\Pr[M(x) = L(x)] \ge \frac{2}{3}$ for all $x$. Also, $L \in \RL$ if there is an $O(\log(n))$-space PTM $N$ such that if $x \in L$, then $\Pr[N(x) = 1] \ge \frac{2}{3}$ and if $x \notin L$, then $\Pr[N(x) = 1] = 0$. 
\end{definition}
Basically, all that we have changed is that we use $\log(n)$-space instead of poly-time. In fact, the very famous problem $\UPATH$ (undirected paths in graphs between two vertices) is in $\NL$ and $\RP$:
\begin{theorem}
$\UPATH \in \RL$.
\end{theorem}
A recent paper actually improves this to $\L$, but is too detailed to present here.
\begin{proof}
We are given a start vertex $s$ and a target $t$. We have a variable vertex $v$, initialized to be $s$. For each step, choose a random neighbor of $v$, $u$, and set $v$ to be $u$. We use log space to store a counter of how many steps we have taken, as well as the index of the current vertex, and what neighbor to choose next. We accept if and only if we reach $t$ within $\ell$ steps (specified later). If $s$ is connected to $t$, then the expected number of steps is at most $10n^4$ (Exercise 7.11 in the book); therefore, our algorithm accepts with probability $\ge \frac{3}{4}$. 
\end{proof}

\subsection*{Exercises}
\newcommand{\BPPpoly}{\BPP/\poly}
\begin{enumerate}
\item Show that $\BPPpoly = \Ppoly$. % http://zoo.cs.yale.edu/classes/cs468/fall12/468Solutions2.pdf
\item Show that if $\NP \subseteq \BPP$, then $\NP = \RP$. % http://www.inf.ed.ac.uk/teaching/courses/cmc/cw3_solns.pdf
\end{enumerate}